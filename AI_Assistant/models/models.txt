How to Set Up the Offline LLM for NEURO

1. Install the required Python package:
   pip install llama-cpp-python

2. Download an offline model in GGUF format.

3. Place the model file in the models folder:
   AI_Assistant/models/tinyllama-1.1b-chat.Q4_K_M.gguf

4. Enable the LLM in the configuration file config/settings.json by setting:
   {
     "assistant_name": "NEURO",
     "user_name": "User",
     "tts": true,
     "use_llm": true,
     "model": "tinyllama-1.1b-chat-v1.0-q4_k_m.gguf",
     "context_size": 2048
   }

5. Run the assistant. If everything works, you should see:
   [LLM] Offline LLM loaded successfully
   NEURO online.
